{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae09ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda4c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6b395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "config = SparkConf().setMaster('local[4]').setAppName(\"ETLSession\")\n",
    "sc = SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e72bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ETL Pipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7348c6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETLSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f36047ca320>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f5e1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETLSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=ETLSession>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1962045",
   "metadata": {},
   "outputs": [],
   "source": [
    "hremployeeDF = spark.read.format('jdbc')\\\n",
    ".option('url','jdbc:mysql://localhost:3306/hremployeeDB')\\\n",
    ".option('dbtable', 'HR_Employee')\\\n",
    ".option('user', 'root')\\\n",
    ".option('password', 'hadoop@123')\\\n",
    ".option('driver', 'com.mysql.cj.jdbc.Driver')\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12439096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1469"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hremployeeDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46894db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(HR_Employee) [numPartitions=1] [EmployeeID#0,Department#1,JobRole#2,Attrition#3,Gender#4,Age#5,MaritalStatus#6,Education#7,EducationField#8,BusinessTravel#9,JobInvolvement#10,JobLevel#11,JobSatisfaction#12,Hourlyrate#13,Income#14,Salaryhike#15,OverTime#16,Workex#17,YearsSinceLastPromotion#18,EmpSatisfaction#19,TrainingTimesLastYear#20,WorkLifeBalance#21,Performance_Rating#22] PushedFilters: [], ReadSchema: struct<EmployeeID:int,Department:string,JobRole:string,Attrition:string,Gender:string,Age:int,Mar...\n"
     ]
    }
   ],
   "source": [
    "# show physical plan of execution which is known as DAG\n",
    "hremployeeDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98433911",
   "metadata": {},
   "source": [
    "#### Create materialized view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e9bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "hremployeeDF.createOrReplaceTempView('hremployee')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b6bbe",
   "metadata": {},
   "source": [
    "#### 1. Display shape of hremployee table\n",
    "- show number of rows number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bfab512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|rows|cols|\n",
      "+----+----+\n",
      "|1469|  23|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_cols = len(hremployeeDF.columns)\n",
    "# number of rows:\n",
    "spark.sql(f\"\"\"\n",
    "select count(*) as rows, {no_cols} as cols\n",
    "from hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "113982f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            col_name|\n",
      "+--------------------+\n",
      "|          EmployeeID|\n",
      "|          Department|\n",
      "|             JobRole|\n",
      "|           Attrition|\n",
      "|              Gender|\n",
      "|                 Age|\n",
      "|       MaritalStatus|\n",
      "|           Education|\n",
      "|      EducationField|\n",
      "|      BusinessTravel|\n",
      "|      JobInvolvement|\n",
      "|            JobLevel|\n",
      "|     JobSatisfaction|\n",
      "|          Hourlyrate|\n",
      "|              Income|\n",
      "|          Salaryhike|\n",
      "|            OverTime|\n",
      "|              Workex|\n",
      "|YearsSinceLastPro...|\n",
      "|     EmpSatisfaction|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    " SHOW COLUMNS IN hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "150eaba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hremployeeDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "24f9696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|          EmployeeID|      int|   null|\n",
      "|          Department|   string|   null|\n",
      "|             JobRole|   string|   null|\n",
      "|           Attrition|   string|   null|\n",
      "|              Gender|   string|   null|\n",
      "|                 Age|      int|   null|\n",
      "|       MaritalStatus|   string|   null|\n",
      "|           Education|   string|   null|\n",
      "|      EducationField|   string|   null|\n",
      "|      BusinessTravel|   string|   null|\n",
      "|      JobInvolvement|   string|   null|\n",
      "|            JobLevel|      int|   null|\n",
      "|     JobSatisfaction|   string|   null|\n",
      "|          Hourlyrate|      int|   null|\n",
      "|              Income|      int|   null|\n",
      "|          Salaryhike|      int|   null|\n",
      "|            OverTime|   string|   null|\n",
      "|              Workex|      int|   null|\n",
      "|YearsSinceLastPro...|      int|   null|\n",
      "|     EmpSatisfaction|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe hremployee').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865df56",
   "metadata": {},
   "source": [
    "#### 2. write a query to show first three employee from each job role to join the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e08ffee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+---+\n",
      "|EmployeeID|             JobRole|Workex|rnk|\n",
      "+----------+--------------------+------+---+\n",
      "|         1|     Sales Executive|     8|  1|\n",
      "|        28|     Sales Executive|    10|  2|\n",
      "|        40|     Sales Executive|    10|  3|\n",
      "|         9|Manufacturing Dir...|    10|  1|\n",
      "|        16|Manufacturing Dir...|    10|  2|\n",
      "|        21|Manufacturing Dir...|     5|  3|\n",
      "|         3|Laboratory Techni...|     7|  1|\n",
      "|         5|Laboratory Techni...|     6|  2|\n",
      "|         6|Laboratory Techni...|     8|  3|\n",
      "|        22|Sales Representative|    10|  1|\n",
      "|        34|Sales Representative|    19|  2|\n",
      "|        37|Sales Representative|     3|  3|\n",
      "|        10|Healthcare Repres...|    17|  1|\n",
      "|        29|Healthcare Repres...|    24|  2|\n",
      "|        32|Healthcare Repres...|     9|  3|\n",
      "|         2|  Research Scientist|    10|  1|\n",
      "|         4|  Research Scientist|     8|  2|\n",
      "|        13|  Research Scientist|     5|  3|\n",
      "|        19|             Manager|    31|  1|\n",
      "|        26|             Manager|    26|  2|\n",
      "+----------+--------------------+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *\n",
    "from (\n",
    "select \n",
    "    EmployeeID,\n",
    "    JobRole,\n",
    "    Workex,\n",
    "    rank() over (partition by JobRole order by EmployeeID) as rnk\n",
    "from hremployee\n",
    ") as _\n",
    "where rnk <= 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb8668",
   "metadata": {},
   "source": [
    "#### 3. write a query to show top three employees from each job role earning highest salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "62e1c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+----+\n",
      "|EmployeeID|             JobRole|Income|rank|\n",
      "+----------+--------------------+------+----+\n",
      "|        99|     Sales Executive| 13872|   1|\n",
      "|       545|     Sales Executive| 13770|   2|\n",
      "|       839|     Sales Executive| 13758|   3|\n",
      "|       722|Manufacturing Dir...| 13973|   1|\n",
      "|       628|Manufacturing Dir...| 13826|   2|\n",
      "|       744|Manufacturing Dir...| 13726|   3|\n",
      "|       678|Laboratory Techni...|  7403|   1|\n",
      "|       817|Laboratory Techni...|  6782|   2|\n",
      "|       945|Laboratory Techni...|  6674|   3|\n",
      "|       565|Sales Representative|  6632|   1|\n",
      "|      1308|Sales Representative|  5405|   2|\n",
      "|      1220|Sales Representative|  4502|   3|\n",
      "|      1181|Healthcare Repres...| 13966|   1|\n",
      "|       317|Healthcare Repres...| 13964|   2|\n",
      "|       190|Healthcare Repres...| 13734|   3|\n",
      "|        68|  Research Scientist|  9724|   1|\n",
      "|      1315|  Research Scientist|  6962|   2|\n",
      "|      1305|  Research Scientist|  6854|   3|\n",
      "|       191|             Manager| 19999|   1|\n",
      "|       852|             Manager| 19943|   2|\n",
      "+----------+--------------------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *\n",
    "from (\n",
    "select \n",
    "    EmployeeID,\n",
    "    JobRole,\n",
    "    Income,\n",
    "    rank() over (partition by JobRole order by Income desc) as rank\n",
    "from hremployee\n",
    ") as _\n",
    "where rank <= 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af8c2d",
   "metadata": {},
   "source": [
    "#### 4. Show top 3 highest package from overall job role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de06a7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------------------+------+\n",
      "|EmployeeID|          JobRole|          Department|Income|\n",
      "+----------+-----------------+--------------------+------+\n",
      "|       191|          Manager|Research & Develo...| 19999|\n",
      "|       747|Research Director|Research & Develo...| 19973|\n",
      "|       852|          Manager|Research & Develo...| 19943|\n",
      "+----------+-----------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select EmployeeID, JobRole, Department, Income\n",
    "from hremployee\n",
    "order by Income desc\n",
    "limit 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f32dd9",
   "metadata": {},
   "source": [
    "#### 5.  write a SQL query to show employee in ascending order with respect to employee income compared with previous income for each job role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c48bc3c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+-----------+-----------+\n",
      "|EmployeeID|             JobRole|Income|prev_income|income_diff|\n",
      "+----------+--------------------+------+-----------+-----------+\n",
      "|      1466|Healthcare Repres...|  9991|          0|      -9991|\n",
      "|       269|Healthcare Repres...| 13496|       4741|      -8755|\n",
      "|      1181|Healthcare Repres...| 13966|       6842|      -7124|\n",
      "|      1138|Healthcare Repres...| 11245|       4148|      -7097|\n",
      "|       190|Healthcare Repres...| 13734|       6673|      -7061|\n",
      "|       675|Healthcare Repres...| 10552|       4014|      -6538|\n",
      "|       376|Healthcare Repres...| 10965|       4522|      -6443|\n",
      "|       814|Healthcare Repres...| 12169|       5731|      -6438|\n",
      "|      1055|Healthcare Repres...| 10466|       4035|      -6431|\n",
      "|       737|Healthcare Repres...| 10999|       4777|      -6222|\n",
      "|       730|Healthcare Repres...| 10388|       4240|      -6148|\n",
      "|      1093|Healthcare Repres...| 10124|       4069|      -6055|\n",
      "|        65|Healthcare Repres...| 10096|       4152|      -5944|\n",
      "|       472|Healthcare Repres...|  9824|       4089|      -5735|\n",
      "|       915|Healthcare Repres...| 13577|       7978|      -5599|\n",
      "|        94|Healthcare Repres...| 10673|       5163|      -5510|\n",
      "|       252|Healthcare Repres...| 10938|       5582|      -5356|\n",
      "|      1221|Healthcare Repres...| 10748|       5562|      -5186|\n",
      "|       553|Healthcare Repres...| 11103|       6811|      -4292|\n",
      "|      1035|Healthcare Repres...| 10851|       6651|      -4200|\n",
      "+----------+--------------------+------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select\n",
    "    *,\n",
    "    prev_income - Income as income_diff\n",
    "from (\n",
    "select \n",
    "    EmployeeID,\n",
    "    JobRole,\n",
    "    Income,\n",
    "    lag(Income, 1, 0) over (partition by JobRole order by EmployeeID desc) as prev_income\n",
    "from hremployee\n",
    ") as _\n",
    "order by JobRole, income_diff\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e4a9753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+-----------+\n",
      "|EmployeeID|             JobRole|Income|income_diff|\n",
      "+----------+--------------------+------+-----------+\n",
      "|      1466|Healthcare Repres...|  9991|      -9991|\n",
      "|       269|Healthcare Repres...| 13496|      -8755|\n",
      "|      1181|Healthcare Repres...| 13966|      -7124|\n",
      "|      1138|Healthcare Repres...| 11245|      -7097|\n",
      "|       190|Healthcare Repres...| 13734|      -7061|\n",
      "|       675|Healthcare Repres...| 10552|      -6538|\n",
      "|       376|Healthcare Repres...| 10965|      -6443|\n",
      "|       814|Healthcare Repres...| 12169|      -6438|\n",
      "|      1055|Healthcare Repres...| 10466|      -6431|\n",
      "|       737|Healthcare Repres...| 10999|      -6222|\n",
      "|       730|Healthcare Repres...| 10388|      -6148|\n",
      "|      1093|Healthcare Repres...| 10124|      -6055|\n",
      "|        65|Healthcare Repres...| 10096|      -5944|\n",
      "|       472|Healthcare Repres...|  9824|      -5735|\n",
      "|       915|Healthcare Repres...| 13577|      -5599|\n",
      "|        94|Healthcare Repres...| 10673|      -5510|\n",
      "|       252|Healthcare Repres...| 10938|      -5356|\n",
      "|      1221|Healthcare Repres...| 10748|      -5186|\n",
      "|       553|Healthcare Repres...| 11103|      -4292|\n",
      "|      1035|Healthcare Repres...| 10851|      -4200|\n",
      "+----------+--------------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "    EmployeeID,\n",
    "    JobRole,\n",
    "    Income,\n",
    "    lag(Income, 1, 0) over (partition by JobRole order by EmployeeID desc)  - Income as income_diff\n",
    "from hremployee\n",
    "order by JobRole, income_diff\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f3554b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+---------+------+---+-------------+---------+----------------+--------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "|EmployeeID|          Department|           JobRole|Attrition|Gender|Age|MaritalStatus|Education|  EducationField|BusinessTravel|JobInvolvement|JobLevel|JobSatisfaction|Hourlyrate|Income|Salaryhike|OverTime|Workex|YearsSinceLastPromotion|EmpSatisfaction|TrainingTimesLastYear|WorkLifeBalance|Performance_Rating|\n",
      "+----------+--------------------+------------------+---------+------+---+-------------+---------+----------------+--------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "|       721|Research & Develo...|Research Scientist|      Yes|Female| 30|      Married| Bachelor|   Life Sciences| Travel_Rarely|          High|       1|           High|        48|  2132|        11|     Yes|     7|                      0|            Low|                    2|         Better|        Excellent\r",
      "|\n",
      "|      1092|Research & Develo...|Research Scientist|       No|  Male| 45|      Married| Bachelor|Technical Degree| Travel_Rarely|          High|       1|      Very High|        97|  2132|        20|      No|     8|                      0|      Very High|                    3|         Better|      Outstanding\r",
      "|\n",
      "+----------+--------------------+------------------+---------+------+---+-------------+---------+----------------+--------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select\n",
    "    *\n",
    "from hremployee\n",
    "where Income = 2132\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab56c8",
   "metadata": {},
   "source": [
    "#### 6. lead()\n",
    "- Row's next records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "80be9ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------+---+------+------+------+-----------+\n",
      "|EmployeeID|Department|        JobRole|age|gender|Income|workex|next_income|\n",
      "+----------+----------+---------------+---+------+------+------+-----------+\n",
      "|         1|     Sales|Sales Executive| 41|Female|  5993|     8|       5376|\n",
      "|        28|     Sales|Sales Executive| 42|  Male|  6825|    10|       8726|\n",
      "|        40|     Sales|Sales Executive| 33|Female|  5376|    10|       4568|\n",
      "|        44|     Sales|Sales Executive| 27|  Male|  8726|     9|       5772|\n",
      "|        47|     Sales|Sales Executive| 34|  Male|  4568|    10|       5454|\n",
      "|        49|     Sales|Sales Executive| 46|  Male|  5772|    14|       4157|\n",
      "|        53|     Sales|Sales Executive| 44|Female|  5454|     9|       9069|\n",
      "|        55|     Sales|Sales Executive| 26|Female|  4157|     5|       7637|\n",
      "|        57|     Sales|Sales Executive| 35|  Male|  9069|     9|       5473|\n",
      "|        64|     Sales|Sales Executive| 59|Female|  7637|    28|       4312|\n",
      "|        71|     Sales|Sales Executive| 59|Female|  5473|    20|      10239|\n",
      "|        77|     Sales|Sales Executive| 35|  Male|  4312|    16|       9619|\n",
      "|        83|     Sales|Sales Executive| 55|  Male| 10239|    24|       5441|\n",
      "|        90|     Sales|Sales Executive| 46|  Male|  9619|     9|       5209|\n",
      "|        92|     Sales|Sales Executive| 51|  Male|  5441|    11|       5010|\n",
      "|        93|     Sales|Sales Executive| 30|Female|  5209|    11|       4999|\n",
      "|        95|     Sales|Sales Executive| 32|  Male|  5010|    12|       4221|\n",
      "|        97|     Sales|Sales Executive| 24|Female|  4999|     4|      13872|\n",
      "|        98|     Sales|Sales Executive| 28|  Male|  4221|     5|       5744|\n",
      "|        99|     Sales|Sales Executive| 58|  Male| 13872|    38|       7428|\n",
      "+----------+----------+---------------+---+------+------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select EmployeeID, Department, JobRole, age, gender, Income, workex,\n",
    "lead(Income, 2, 0) over(partition by JobRole order by EmployeeID) as next_income\n",
    "from hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20e1f4",
   "metadata": {},
   "source": [
    "#### 7. NTILE():\n",
    "- dividing records into a number of quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3e7722a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---+------+------+------+-----------+\n",
      "|EmployeeID|          Department|             JobRole|age|gender|Income|workex|income_tile|\n",
      "+----------+--------------------+--------------------+---+------+------+------+-----------+\n",
      "|       514|Research & Develo...|  Research Scientist| 20|  Male|  1009|     1|          1|\n",
      "|       728|Research & Develo...|  Research Scientist| 18|  Male|  1051|     0|          1|\n",
      "|       765|               Sales|Sales Representative| 28|  Male|  1052|     1|          1|\n",
      "|      1338|               Sales|Sales Representative| 30|  Male|  1081|     1|          1|\n",
      "|      1365|               Sales|Sales Representative| 29|  Male|  1091|     1|          1|\n",
      "|       178|Research & Develo...|Laboratory Techni...| 19|  Male|  1102|     1|          1|\n",
      "|       912|               Sales|Sales Representative| 25|  Male|  1118|     1|          1|\n",
      "|      1402|Research & Develo...|Laboratory Techni...| 31|Female|  1129|     1|          1|\n",
      "|       302|               Sales|Sales Representative| 18|Female|  1200|     0|          1|\n",
      "|       911|Research & Develo...|  Research Scientist| 23|  Male|  1223|     1|          1|\n",
      "|        24|Research & Develo...|  Research Scientist| 21|  Male|  1232|     0|          1|\n",
      "|      1017|Research & Develo...|  Research Scientist| 31|Female|  1261|     1|          1|\n",
      "|      1053|Research & Develo...|  Research Scientist| 30|  Male|  1274|     1|          1|\n",
      "|       516|Research & Develo...|Laboratory Techni...| 35|  Male|  1281|     1|          1|\n",
      "|      1013|               Sales|Sales Representative| 31|Female|  1359|     1|          1|\n",
      "|      1205|Research & Develo...|Laboratory Techni...| 32|  Male|  1393|     1|          1|\n",
      "|       778|Research & Develo...|Laboratory Techni...| 21|Female|  1416|     1|          1|\n",
      "|       297|Research & Develo...|Laboratory Techni...| 18|  Male|  1420|     0|          1|\n",
      "|       150|Research & Develo...|Laboratory Techni...| 19|Female|  1483|     1|          1|\n",
      "|      1311|Research & Develo...|  Research Scientist| 18|Female|  1514|     0|          1|\n",
      "+----------+--------------------+--------------------+---+------+------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select EmployeeID, Department, JobRole, age, gender, Income, workex,\n",
    "ntile(4) over(order by Income) as income_tile\n",
    "from hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c196583",
   "metadata": {},
   "source": [
    "#### 8. find the number of employees in each percentile group 0-25th 25th-50th, 50th-75th, 75th-100th, using percent rank and case when to create category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0546e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|  category|count|\n",
      "+----------+-----+\n",
      "|  0th-25th|  367|\n",
      "| 25th-50th|  366|\n",
      "| 50th-75th|  367|\n",
      "|75th-100th|  369|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select\n",
    "    case when percentrank < 0.25 then '0th-25th'\n",
    "        when percentrank < 0.50 then '25th-50th'\n",
    "        when percentrank < 0.75 then '50th-75th'\n",
    "        else '75th-100th'\n",
    "    end as category,\n",
    "    count(*) as count\n",
    "from (\n",
    "select\n",
    "    EmployeeID,\n",
    "    percent_rank() over(partition by Department order by Income) as percentrank\n",
    "from hremployee\n",
    ") as _\n",
    "group by category\n",
    "order by category\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d3dcb",
   "metadata": {},
   "source": [
    "#### Hive intergration with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f551b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close pyspark and start again and start from here\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba8392d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21186 SecondaryNameNode\n",
      "21346 ResourceManager\n",
      "21511 NodeManager\n",
      "20760 NameNode\n",
      "20955 DataNode\n",
      "22588 SparkSubmit\n",
      "22671 Jps\n"
     ]
    }
   ],
   "source": [
    "# make sure every service is running\n",
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925c9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark integration with Hive Warehouse\n",
    "# config name for Hive Integration \"spark.sql.warehouse.dir\" value=/usr/hive/warehouse\n",
    "spark = (SparkSession.builder.appName(\"pyspak-Hive-Integration\")\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/usr/hive/warehouse\")\n",
    "        .enableHiveSupport().getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2cd9475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d301810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspak-Hive-Integration</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3a38132320>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e79a9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "create database if not exists airlines\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644a0505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "use airlines\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead84532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|    airlines|\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "show databases\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a584f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "show tables\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11796a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sql_statement = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS flights (\n",
    "    DayofMonth INT,\n",
    "    DayofWeek INT,\n",
    "    Carrier STRING,\n",
    "    OriginAirportId INT,\n",
    "    DestAirportId INT,\n",
    "    DepDelay INT,\n",
    "    ArrDelay INT\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE\n",
    "TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b76cbe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|airlines| airports|      false|\n",
      "|airlines|  flights|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f17bbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "load data local inpath '/home/hadoop/Downloads/raw_flight_data.csv' overwrite into table flights\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b218d313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sql_statement = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS airports(\n",
    "   airport_id int,\n",
    "   city varchar(50),\n",
    "   state varchar(50),\n",
    "   name varchar(50)\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE\n",
    "TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d195fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "load data local inpath '/home/hadoop/Downloads/airports.csv' overwrite into table airports\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf3884bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|airlines| airports|      false|\n",
      "|airlines|  flights|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2db2fe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1048575|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*) from flights\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23dd937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     365|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*) from airports\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42a9982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+--------------------+\n",
      "|airport_id|       city|state|                name|\n",
      "+----------+-----------+-----+--------------------+\n",
      "|     10165|Adak Island|   AK|                Adak|\n",
      "|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n",
      "|     10304|      Aniak|   AK|       Aniak Airport|\n",
      "|     10754|     Barrow|   AK|Wiley Post/Will R...|\n",
      "|     10551|     Bethel|   AK|      Bethel Airport|\n",
      "+----------+-----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from airports\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57fef239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayofWeek|Carrier|OriginAirportId|DestAirportId|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "|        19|        5|     DL|          15016|        11433|      28|      24|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|     -11|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from flights\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2555b",
   "metadata": {},
   "source": [
    "#### 1. Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de3b3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = spark.table(\"airlines.flights\")\n",
    "airports_df = spark.table(\"airlines.airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c764dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+--------------------+\n",
      "|airport_id|       city|state|                name|\n",
      "+----------+-----------+-----+--------------------+\n",
      "|     10165|Adak Island|   AK|                Adak|\n",
      "|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n",
      "|     10304|      Aniak|   AK|       Aniak Airport|\n",
      "|     10754|     Barrow|   AK|Wiley Post/Will R...|\n",
      "|     10551|     Bethel|   AK|      Bethel Airport|\n",
      "+----------+-----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39dcc83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayofWeek|Carrier|OriginAirportId|DestAirportId|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "|        19|        5|     DL|          15016|        11433|      28|      24|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|     -11|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f20f9a",
   "metadata": {},
   "source": [
    "#### 2. Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d173df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join = flights_df.join(\n",
    "    airports_df, \n",
    "    on=flights_df.OriginAirportId == airports_df.airport_id, \n",
    "    how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "643cdef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+--------------------+\n",
      "|DayofMonth|DayofWeek|Carrier|OriginAirportId|DestAirportId|DepDelay|ArrDelay|airport_id|          city|state|                name|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+--------------------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|     11433|       Detroit|   MI|Detroit Metro Way...|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|     14869|Salt Lake City|   UT|Salt Lake City In...|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|     14057|      Portland|   OR|Portland Internat...|\n",
      "|        19|        5|     DL|          15016|        11433|      28|      24|     15016|     St. Louis|   MO|Lambert-St. Louis...|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|     -11|     11193|    Cincinnati|   OH|Cincinnati/Northe...|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_join.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c919d0d",
   "metadata": {},
   "source": [
    "#### 3. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5705bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join = flights_join.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc9d7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write it into local as parquet\n",
    "flights_join.write.parquet('file:///home/hadoop/Downloads/flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc7345a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from local parquet file into datafile\n",
    "flights_parquet_df = spark.read.parquet('file:///home/hadoop/Downloads/flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f75c8106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+---------+-----+--------------------+\n",
      "|DayofMonth|DayofWeek|Carrier|OriginAirportId|DestAirportId|DepDelay|ArrDelay|airport_id|     city|state|                name|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+---------+-----+--------------------+\n",
      "|        20|        1|     WN|          14908|        13796|      -1|     -10|     14908|Santa Ana|   CA|John Wayne Airpor...|\n",
      "|         9|        7|     WN|          14831|        13891|      -1|      -3|     14831| San Jose|   CA|Norman Y. Mineta ...|\n",
      "|         8|        1|     EV|          13931|        12266|      -5|     -24|     13931|  Norfolk|   VA|Norfolk Internati...|\n",
      "|        31|        5|     UA|          11618|        10721|      36|      25|     11618|   Newark|   NJ|Newark Liberty In...|\n",
      "|        13|        6|     WN|          13871|        12889|      -3|      -9|     13871|    Omaha|   NE|     Eppley Airfield|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+---------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_parquet_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8570ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data as parquet into hdfs\n",
    "flights_join.write.parquet('/flights1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45b4bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data as parquet into hdfs by partitioning it by carriers\n",
    "flights_join.write.partitionBy('Carrier').parquet('/flights2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39c09c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+---------+-----+--------------------+\n",
      "|DayofMonth|DayofWeek|Carrier|OriginAirportId|DestAirportId|DepDelay|ArrDelay|airport_id|     city|state|                name|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+---------+-----+--------------------+\n",
      "|        20|        1|     WN|          14908|        13796|      -1|     -10|     14908|Santa Ana|   CA|John Wayne Airpor...|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+---------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_join.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1d3a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.bucketBy(col='state', numBuckets = 50).format('csv').saveAsTable('bucketed_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86766578",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.partitionBy('Carrier').bucketBy(col='state', numBuckets = 50).format('parquet').saveAsTable('part_bucket_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51f6f00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|carrier|count(1)|\n",
      "+-------+--------+\n",
      "|     UA|  122443|\n",
      "|     AA|  124037|\n",
      "|     EV|   46563|\n",
      "|     B6|   51381|\n",
      "|     DL|  134724|\n",
      "|     OO|   69785|\n",
      "|     F9|    9811|\n",
      "|     YV|   14612|\n",
      "|     US|  100668|\n",
      "|     MQ|   45926|\n",
      "|     HA|    4962|\n",
      "|     AS|   28796|\n",
      "|     FL|   28053|\n",
      "|     VX|   14683|\n",
      "|     WN|  216101|\n",
      "|     9E|   36030|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select carrier, count(*) from part_bucket_table group by carrier').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "50e93b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/hive/warehouse/airlines.db/bucketed_table\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -find / -name bucketed_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a592f63",
   "metadata": {},
   "source": [
    "#### Load on MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3014cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_properties = {\n",
    "    'user': 'root',\n",
    "    'password': 'hadoop@123',\n",
    "    'driver': 'com.mysql.cj.jdbc.Driver'\n",
    "}\n",
    "\n",
    "flights_join.write.jdbc(\n",
    "    url='jdbc:mysql://localhost:3306/flights', \n",
    "    table='airlines', \n",
    "    mode='overwrite',\n",
    "    properties=connection_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663c74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0612c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d169f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
